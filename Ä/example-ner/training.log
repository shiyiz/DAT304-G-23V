2023-04-20 00:57:14,781 ----------------------------------------------------------------------------------------------------
2023-04-20 00:57:14,781 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): WordEmbeddings(
      'glove'
      (embedding): Embedding(400001, 100)
    )
    (list_embedding_1): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.05, inplace=False)
        (encoder): Embedding(300, 100)
        (rnn): LSTM(100, 2048)
      )
    )
    (list_embedding_2): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.05, inplace=False)
        (encoder): Embedding(300, 100)
        (rnn): LSTM(100, 2048)
      )
    )
    (list_embedding_3): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.25, inplace=False)
        (encoder): Embedding(275, 100)
        (rnn): LSTM(100, 2048)
      )
    )
    (list_embedding_4): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.25, inplace=False)
        (encoder): Embedding(275, 100)
        (rnn): LSTM(100, 2048)
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=8292, out_features=8292, bias=True)
  (rnn): LSTM(8292, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=87, bias=True)
  (loss_function): ViterbiLoss()
  (crf): CRF()
)"
2023-04-20 00:57:14,783 ----------------------------------------------------------------------------------------------------
2023-04-20 00:57:14,784 Corpus: "Corpus: 140 train + 43 dev + 55 test sentences"
2023-04-20 00:57:14,784 ----------------------------------------------------------------------------------------------------
2023-04-20 00:57:14,784 Parameters:
2023-04-20 00:57:14,784  - learning_rate: "0.100000"
2023-04-20 00:57:14,784  - mini_batch_size: "4"
2023-04-20 00:57:14,784  - patience: "3"
2023-04-20 00:57:14,785  - anneal_factor: "0.5"
2023-04-20 00:57:14,785  - max_epochs: "2000"
2023-04-20 00:57:14,785  - shuffle: "True"
2023-04-20 00:57:14,785  - train_with_dev: "False"
2023-04-20 00:57:14,785  - batch_growth_annealing: "False"
2023-04-20 00:57:14,785 ----------------------------------------------------------------------------------------------------
2023-04-20 00:57:14,785 Model training base path: "D:Ã„\example-ner"
2023-04-20 00:57:14,785 ----------------------------------------------------------------------------------------------------
2023-04-20 00:57:14,786 Device: cpu
2023-04-20 00:57:14,786 ----------------------------------------------------------------------------------------------------
2023-04-20 00:57:14,786 Embeddings storage mode: cpu
2023-04-20 00:57:14,786 ----------------------------------------------------------------------------------------------------
2023-04-20 00:57:18,713 epoch 1 - iter 3/35 - loss 4.84059255 - time (sec): 3.93 - samples/sec: 41.51 - lr: 0.100000
2023-04-20 00:57:24,693 epoch 1 - iter 6/35 - loss 4.32387449 - time (sec): 9.91 - samples/sec: 46.23 - lr: 0.100000
2023-04-20 00:57:31,087 epoch 1 - iter 9/35 - loss 4.00075597 - time (sec): 16.30 - samples/sec: 51.16 - lr: 0.100000
2023-04-20 00:57:39,132 epoch 1 - iter 12/35 - loss 3.85213920 - time (sec): 24.35 - samples/sec: 51.34 - lr: 0.100000
2023-04-20 00:57:46,926 epoch 1 - iter 15/35 - loss 3.57198735 - time (sec): 32.14 - samples/sec: 51.56 - lr: 0.100000
2023-04-20 00:57:58,366 epoch 1 - iter 18/35 - loss 3.53765047 - time (sec): 43.58 - samples/sec: 50.53 - lr: 0.100000
2023-04-20 00:58:07,931 epoch 1 - iter 21/35 - loss 3.42042868 - time (sec): 53.14 - samples/sec: 50.41 - lr: 0.100000
2023-04-20 00:58:16,256 epoch 1 - iter 24/35 - loss 3.37440005 - time (sec): 61.47 - samples/sec: 50.94 - lr: 0.100000
2023-04-20 00:58:22,235 epoch 1 - iter 27/35 - loss 3.32290780 - time (sec): 67.45 - samples/sec: 50.91 - lr: 0.100000
2023-04-20 00:58:32,374 epoch 1 - iter 30/35 - loss 3.28416126 - time (sec): 77.59 - samples/sec: 50.51 - lr: 0.100000
2023-04-20 00:58:42,522 epoch 1 - iter 33/35 - loss 3.27486885 - time (sec): 87.74 - samples/sec: 50.31 - lr: 0.100000
2023-04-20 00:58:49,496 ----------------------------------------------------------------------------------------------------
2023-04-20 00:58:49,496 EPOCH 1 done: loss 3.2687 - lr 0.100000
2023-04-20 00:59:15,223 Evaluating as a multi-label problem: False
2023-04-20 00:59:15,236 DEV : loss 3.003836154937744 - f1-score (micro avg)  0.0
2023-04-20 00:59:15,246 BAD EPOCHS (no improvement): 0
2023-04-20 00:59:15,247 saving best model
