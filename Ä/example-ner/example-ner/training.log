2023-04-20 00:54:39,422 ----------------------------------------------------------------------------------------------------
2023-04-20 00:54:39,423 Model: "SequenceTagger(
  (embeddings): StackedEmbeddings(
    (list_embedding_0): WordEmbeddings(
      'glove'
      (embedding): Embedding(400001, 100)
    )
    (list_embedding_1): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.05, inplace=False)
        (encoder): Embedding(300, 100)
        (rnn): LSTM(100, 2048)
      )
    )
    (list_embedding_2): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.05, inplace=False)
        (encoder): Embedding(300, 100)
        (rnn): LSTM(100, 2048)
      )
    )
    (list_embedding_3): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.25, inplace=False)
        (encoder): Embedding(275, 100)
        (rnn): LSTM(100, 2048)
      )
    )
    (list_embedding_4): FlairEmbeddings(
      (lm): LanguageModel(
        (drop): Dropout(p=0.25, inplace=False)
        (encoder): Embedding(275, 100)
        (rnn): LSTM(100, 2048)
      )
    )
  )
  (word_dropout): WordDropout(p=0.05)
  (locked_dropout): LockedDropout(p=0.5)
  (embedding2nn): Linear(in_features=8292, out_features=8292, bias=True)
  (rnn): LSTM(8292, 256, batch_first=True, bidirectional=True)
  (linear): Linear(in_features=512, out_features=87, bias=True)
  (loss_function): ViterbiLoss()
  (crf): CRF()
)"
2023-04-20 00:54:39,426 ----------------------------------------------------------------------------------------------------
2023-04-20 00:54:39,426 Corpus: "Corpus: 140 train + 43 dev + 55 test sentences"
2023-04-20 00:54:39,426 ----------------------------------------------------------------------------------------------------
2023-04-20 00:54:39,427 Parameters:
2023-04-20 00:54:39,427  - learning_rate: "0.100000"
2023-04-20 00:54:39,427  - mini_batch_size: "4"
2023-04-20 00:54:39,427  - patience: "3"
2023-04-20 00:54:39,427  - anneal_factor: "0.5"
2023-04-20 00:54:39,427  - max_epochs: "2000"
2023-04-20 00:54:39,428  - shuffle: "True"
2023-04-20 00:54:39,428  - train_with_dev: "False"
2023-04-20 00:54:39,428  - batch_growth_annealing: "False"
2023-04-20 00:54:39,428 ----------------------------------------------------------------------------------------------------
2023-04-20 00:54:39,428 Model training base path: "D:Ã„\example-ner\example-ner"
2023-04-20 00:54:39,429 ----------------------------------------------------------------------------------------------------
2023-04-20 00:54:39,429 Device: cpu
2023-04-20 00:54:39,429 ----------------------------------------------------------------------------------------------------
2023-04-20 00:54:39,429 Embeddings storage mode: cpu
2023-04-20 00:54:39,429 ----------------------------------------------------------------------------------------------------
2023-04-20 00:54:43,349 epoch 1 - iter 3/35 - loss 4.52719041 - time (sec): 3.92 - samples/sec: 41.59 - lr: 0.100000
2023-04-20 00:54:49,054 epoch 1 - iter 6/35 - loss 3.93030325 - time (sec): 9.62 - samples/sec: 47.59 - lr: 0.100000
2023-04-20 00:54:55,453 epoch 1 - iter 9/35 - loss 3.61930210 - time (sec): 16.02 - samples/sec: 52.05 - lr: 0.100000
2023-04-20 00:55:03,385 epoch 1 - iter 12/35 - loss 3.39316128 - time (sec): 23.96 - samples/sec: 52.18 - lr: 0.100000
2023-04-20 00:55:11,125 epoch 1 - iter 15/35 - loss 3.22784306 - time (sec): 31.70 - samples/sec: 52.28 - lr: 0.100000
2023-04-20 00:55:22,449 epoch 1 - iter 18/35 - loss 3.04113411 - time (sec): 43.02 - samples/sec: 51.19 - lr: 0.100000
2023-04-20 00:55:31,948 epoch 1 - iter 21/35 - loss 2.96583090 - time (sec): 52.52 - samples/sec: 51.01 - lr: 0.100000
2023-04-20 00:55:40,150 epoch 1 - iter 24/35 - loss 2.92154336 - time (sec): 60.72 - samples/sec: 51.56 - lr: 0.100000
2023-04-20 00:55:45,958 epoch 1 - iter 27/35 - loss 2.88963609 - time (sec): 66.53 - samples/sec: 51.62 - lr: 0.100000
2023-04-20 00:55:56,101 epoch 1 - iter 30/35 - loss 2.88343590 - time (sec): 76.67 - samples/sec: 51.11 - lr: 0.100000
2023-04-20 00:56:06,211 epoch 1 - iter 33/35 - loss 2.83655645 - time (sec): 86.78 - samples/sec: 50.86 - lr: 0.100000
2023-04-20 00:56:13,258 ----------------------------------------------------------------------------------------------------
2023-04-20 00:56:13,258 EPOCH 1 done: loss 2.8193 - lr 0.100000
2023-04-20 00:56:39,596 Evaluating as a multi-label problem: False
2023-04-20 00:56:39,612 DEV : loss 2.2674481868743896 - f1-score (micro avg)  0.0343
2023-04-20 00:56:39,621 BAD EPOCHS (no improvement): 0
2023-04-20 00:56:39,622 saving best model
